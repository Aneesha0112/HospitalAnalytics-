{"cells":[{"cell_type":"code","source":["lakehouse_silverTable = \"abfss://Fabric_E2E@onelake.dfs.fabric.microsoft.com/Lakehouse_Silver_.Lakehouse/Tables\"\n","spark.conf.set(\"spark.executorEnv.lakehouse_silverTable\", lakehouse_silverTable)\n","lakehouse_silver_table = spark.conf.get(\"spark.executorEnv.lakehouse_silverTable\")\n","print(lakehouse_silver_table)\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":4,"statement_ids":[4],"state":"finished","livy_statement_state":"available","session_id":"8277413f-ff5e-4bc5-9b8a-496359327ab2","normalized_state":"finished","queued_time":"2025-05-23T19:31:04.6452357Z","session_start_time":null,"execution_start_time":"2025-05-23T19:31:04.646372Z","execution_finish_time":"2025-05-23T19:31:04.9527518Z","parent_msg_id":"224bcec5-7222-4a86-969e-7eae7d8b5d8b"},"text/plain":"StatementMeta(, 8277413f-ff5e-4bc5-9b8a-496359327ab2, 4, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["abfss://Fabric_E2E@onelake.dfs.fabric.microsoft.com/Lakehouse_Silver_.Lakehouse/Tables\n"]}],"execution_count":2,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b67bf38f-6d98-4f81-969b-5374b716bb0c"},{"cell_type":"code","source":["try:\n","    from pyspark.sql.functions import to_timestamp, date_format, col, datediff, row_number, lag, when\n","    from pyspark.sql.window import Window\n","\n","    # Load encounters.csv\n","    enc_df = spark.read.option(\"header\", \"true\").csv(\"Files/raw/encounters.csv\")\n","\n","    # Drop rows with critical nulls\n","    enc_df = enc_df.dropna(subset=['Id', 'PATIENT', 'START', 'STOP', 'ENCOUNTERCLASS'])\n","\n","    # Rename and transform columns\n","    admit_df = enc_df \\\n","        .withColumnRenamed(\"Id\", \"DIM_EncounterId\") \\\n","        .withColumnRenamed(\"PATIENT\", \"DIM_patientId\") \\\n","        .withColumnRenamed(\"ORGANIZATION\", \"DIM_providerId\") \\\n","        .withColumn(\"START_TS\", to_timestamp(\"START\", \"yyyy-MM-dd'T'HH:mm:ssX\")) \\\n","        .withColumn(\"STOP_TS\", to_timestamp(\"STOP\", \"yyyy-MM-dd'T'HH:mm:ssX\")) \\\n","        .withColumn(\"DIM_DateId\", date_format(\"START_TS\", \"yyyyMMdd\"))\n","\n","    # Calculate length of stay (in days)\n","    admit_df = admit_df.withColumn(\"length_of_stay\", datediff(\"STOP_TS\", \"START_TS\"))\n","\n","    # Window spec by patient ordered by admission start time\n","    readmit_window = Window.partitionBy(\"DIM_patientId\").orderBy(\"START_TS\")\n","\n","    # Get previous discharge time for each patient\n","    admit_df = admit_df.withColumn(\"prev_discharge\", lag(\"STOP_TS\").over(readmit_window))\n","\n","    # Add readmitted_flag (1 if readmitted within 30 days, else 0)\n","    admit_df = admit_df.withColumn(\n","        \"readmitted_flag\",\n","        when(\n","            (datediff(col(\"START_TS\"), col(\"prev_discharge\")) <= 30) &\n","            (datediff(col(\"START_TS\"), col(\"prev_discharge\")) > 0),\n","            1\n","        ).otherwise(0)\n","    )\n","\n","    # Add surrogate key\n","    surrogate_key_window = Window.orderBy(\"DIM_patientId\", \"DIM_DateId\")\n","    admit_df = admit_df.withColumn(\"Fact_AdmissionId\", row_number().over(surrogate_key_window))\n","\n","    # Final column selection\n","    fact_admissions = admit_df.select(\n","        \"Fact_AdmissionId\",\n","        \"DIM_EncounterId\",\n","        \"DIM_patientId\",\n","        \"DIM_providerId\",\n","        \"DIM_DateId\",\n","        col(\"ENCOUNTERCLASS\").alias(\"admission_type\"),\n","        \"length_of_stay\",\n","        \"readmitted_flag\",\n","        col(\"DESCRIPTION\").alias(\"discharge_disposition\")\n","    )\n","\n","    # Save to Silver layer\n","    fact_admissions.write \\\n","        .mode(\"overwrite\") \\\n","        .option(\"overwriteSchema\", \"true\") \\\n","        .format(\"delta\") \\\n","        .save(f\"{lakehouse_silver_table}/FactAdmissions\")\n","\n","except Exception as e:\n","    print(f\"❌ Notebook 'Fact_Admissions' failed: {str(e)} — Skipping to next item in pipeline.\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":5,"statement_ids":[5],"state":"finished","livy_statement_state":"available","session_id":"8277413f-ff5e-4bc5-9b8a-496359327ab2","normalized_state":"finished","queued_time":"2025-05-23T19:31:12.7127414Z","session_start_time":null,"execution_start_time":"2025-05-23T19:31:12.7139421Z","execution_finish_time":"2025-05-23T19:31:22.4623036Z","parent_msg_id":"89ba4065-254e-4484-ab01-329df77a349a"},"text/plain":"StatementMeta(, 8277413f-ff5e-4bc5-9b8a-496359327ab2, 5, Finished, Available, Finished)"},"metadata":{}}],"execution_count":3,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3e5ae2ba-0ca1-48e7-81a3-3cef40cdbcdc"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"2aaafc3d-8d61-4a21-99ae-09bb332840ae","known_lakehouses":[{"id":"2aaafc3d-8d61-4a21-99ae-09bb332840ae"}],"default_lakehouse_name":"Lakehouse_E2E","default_lakehouse_workspace_id":"9076d5e4-30ee-441b-8e66-85ac9711a0e2"}}},"nbformat":4,"nbformat_minor":5}