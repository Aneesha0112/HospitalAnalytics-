{"cells":[{"cell_type":"code","source":["lakehouse_silverTable = \"abfss://Fabric_E2E@onelake.dfs.fabric.microsoft.com/Lakehouse_Silver_.Lakehouse/Tables\"\n","spark.conf.set(\"spark.executorEnv.lakehouse_silverTable\", lakehouse_silverTable)\n","lakehouse_silver_table = spark.conf.get(\"spark.executorEnv.lakehouse_silverTable\")\n","print(lakehouse_silver_table)\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"be922e7c-bbcd-4fbe-bb48-ee2e09f9ffb3","normalized_state":"finished","queued_time":"2025-05-23T19:33:32.2612743Z","session_start_time":"2025-05-23T19:33:32.2622067Z","execution_start_time":"2025-05-23T19:33:43.4873823Z","execution_finish_time":"2025-05-23T19:33:43.9271372Z","parent_msg_id":"92630ccc-d941-41c2-8f51-a6c713748445"},"text/plain":"StatementMeta(, be922e7c-bbcd-4fbe-bb48-ee2e09f9ffb3, 3, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["abfss://Fabric_E2E@onelake.dfs.fabric.microsoft.com/Lakehouse_Silver_.Lakehouse/Tables\n"]}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"734d6aed-7469-48c2-955e-de7bfc37a04b"},{"cell_type":"code","source":["try:\n","    from pyspark.sql.functions import to_timestamp, datediff, col, count, sum as sum_, date_format\n","    from pyspark.sql import functions as F\n","\n","    # Load encounters\n","    enc_df = spark.read.option(\"header\", \"true\").csv(\"Files/raw/encounters.csv\")\n","    enc_df = enc_df.dropna(subset=['id', 'patient', 'start'])\n","\n","    # Timestamp conversion & rename columns\n","    enc_transformed = enc_df \\\n","        .withColumnRenamed('id', 'Fact_EncounterId') \\\n","        .withColumnRenamed('patient', 'DIM_patientId') \\\n","        .withColumnRenamed('organization', 'DIM_organizationId') \\\n","        .withColumn(\"START\", to_timestamp(\"start\", \"yyyy-MM-dd'T'HH:mm:ssX\")) \\\n","        .withColumn(\"STOP\", to_timestamp(\"stop\", \"yyyy-MM-dd'T'HH:mm:ssX\"))\n","\n","    # Calculate length of stay and DIM_DateId\n","    enc_transformed = enc_transformed \\\n","        .withColumn(\"length_of_stay\", datediff(\"STOP\", \"START\")) \\\n","        .withColumn(\"DIM_DateId\", date_format(col(\"START\"), \"yyyyMMdd\"))\n","\n","    # Load DIM_Procedures (already cleaned earlier)\n","    proc_df = spark.read.format(\"delta\").load(f\"{lakehouse_silver_table}/DIM_Procedures\")\n","\n","    # Group by procedure ID to compute count and cost\n","    proc_summary = proc_df.groupBy(\"DIM_ProcedureId\").agg(\n","        count(\"*\").alias(\"number_of_procedures\"),\n","        sum_(\"base_cost\").alias(\"visit_cost\")\n","    )\n","\n","    # Join encounters with summarized procedure info\n","    enc_proc = enc_transformed.join(\n","        proc_summary,\n","        enc_transformed.Fact_EncounterId == proc_summary.DIM_ProcedureId,\n","        \"left\"\n","    ).drop(proc_summary.DIM_ProcedureId)\n","\n","    # Fill nulls where no procedures exist\n","    enc_proc = enc_proc.fillna({\n","        'number_of_procedures': 0,\n","        'visit_cost': 0.0\n","    })\n","\n","    # Final fact table\n","    fact_encounter_final = enc_proc.select(\n","        \"Fact_EncounterId\",\n","        \"DIM_patientId\",\n","        \"DIM_organizationId\",\n","        \"DIM_DateId\",\n","        \"length_of_stay\",\n","        \"number_of_procedures\",\n","        \"visit_cost\"\n","    )\n","\n","    # Save to Silver layer\n","    fact_encounter_final.write \\\n","        .mode(\"overwrite\") \\\n","        .format(\"delta\") \\\n","        .save(f\"{lakehouse_silver_table}/FactEncounterTest\")\n","\n","except Exception as e:\n","    print(f\"❌ Notebook 'Fact_Encounter' failed: {str(e)} — Skipping to next item in pipeline.\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":4,"statement_ids":[4],"state":"finished","livy_statement_state":"available","session_id":"be922e7c-bbcd-4fbe-bb48-ee2e09f9ffb3","normalized_state":"finished","queued_time":"2025-05-23T19:38:47.2343244Z","session_start_time":null,"execution_start_time":"2025-05-23T19:38:47.2355929Z","execution_finish_time":"2025-05-23T19:39:07.7462396Z","parent_msg_id":"7a95fb20-acfc-4cb8-88c9-8be40d902550"},"text/plain":"StatementMeta(, be922e7c-bbcd-4fbe-bb48-ee2e09f9ffb3, 4, Finished, Available, Finished)"},"metadata":{}}],"execution_count":2,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d6393886-e6f7-4415-bb3a-66bfe3b03f1a"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"2aaafc3d-8d61-4a21-99ae-09bb332840ae","known_lakehouses":[{"id":"2aaafc3d-8d61-4a21-99ae-09bb332840ae"}],"default_lakehouse_name":"Lakehouse_E2E","default_lakehouse_workspace_id":"9076d5e4-30ee-441b-8e66-85ac9711a0e2"},"environment":{"environmentId":"31fd62f0-7a25-4de6-8968-114832cf2a84","workspaceId":"9076d5e4-30ee-441b-8e66-85ac9711a0e2"}}},"nbformat":4,"nbformat_minor":5}