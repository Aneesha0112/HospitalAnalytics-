{"cells":[{"cell_type":"code","source":["lakehouse_silverTable = \"abfss://Fabric_E2E@onelake.dfs.fabric.microsoft.com/Lakehouse_Silver_.Lakehouse/Tables\"\n","spark.conf.set(\"spark.executorEnv.lakehouse_silverTable\", lakehouse_silverTable)\n","lakehouse_silver_table = spark.conf.get(\"spark.executorEnv.lakehouse_silverTable\")\n","print(lakehouse_silver_table)\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"cdc93cc4-6ac7-4438-97ae-142997d2a9e8","normalized_state":"finished","queued_time":"2025-05-23T19:55:54.765994Z","session_start_time":"2025-05-23T19:55:54.7669753Z","execution_start_time":"2025-05-23T19:56:06.1525976Z","execution_finish_time":"2025-05-23T19:56:06.5491449Z","parent_msg_id":"cc3845ae-cb15-42dd-9e13-ec5ac00ea9f3"},"text/plain":"StatementMeta(, cdc93cc4-6ac7-4438-97ae-142997d2a9e8, 3, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["abfss://Fabric_E2E@onelake.dfs.fabric.microsoft.com/Lakehouse_Silver_.Lakehouse/Tables\n"]}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e37dd14d-bfe3-43a5-9bca-824c0f67a20b"},{"cell_type":"code","source":["try:\n","    from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType\n","    from pyspark.sql.functions import col, row_number\n","    from pyspark.sql.window import Window\n","\n","    # Step 1: Define schema for clarity (optional)\n","    vitals_schema = StructType([\n","        StructField(\"Fact_VitalId\", IntegerType(), True),\n","        StructField(\"DIM_patientId\", StringType(), True),\n","        StructField(\"DIM_DateId\", StringType(), True),\n","        StructField(\"vital_type\", StringType(), True),\n","        StructField(\"vital_value\", DoubleType(), True),\n","        StructField(\"unit\", StringType(), True)\n","    ])\n","\n","    # Step 2: Load CSV (not enforcing schema because of extra columns)\n","    vitals_df = spark.read.option(\"header\", True).csv(\"Files/raw/Data_LabResults.csv\")\n","\n","    # Step 3: Filter essential rows\n","    vitals_filtered = vitals_df.filter(\n","        col(\"vital_type\").isNotNull() & col(\"vital_value\").isNotNull()\n","    )\n","\n","    # Step 4: Add surrogate key\n","    windowSpec = Window.orderBy(\"DIM_patientId\", \"DIM_DateId\")\n","    vitals_ranked = vitals_filtered.withColumn(\"Fact_VitalId\", row_number().over(windowSpec))\n","\n","    # Step 5: Recast to match schema\n","    fact_vitals_df = vitals_ranked.select(\n","        col(\"Fact_VitalId\").cast(\"int\"),\n","        col(\"DIM_patientId\").cast(\"string\"),\n","        col(\"DIM_DateId\").cast(\"string\"),\n","        col(\"vital_type\").cast(\"string\"),\n","        col(\"vital_value\").cast(\"double\"),\n","        col(\"units\").cast(\"string\").alias(\"unit\")\n","    )\n","\n","    # Step 6: Write to Silver layer\n","    fact_vitals_df.write \\\n","        .mode(\"overwrite\") \\\n","        .format(\"delta\") \\\n","        .save(f\"{lakehouse_silver_table}/FactVitals\")\n","\n","except Exception as e:\n","    print(f\"❌ Notebook 'Fact_Vitals' failed: {str(e)} — Skipping to next item in pipeline.\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":4,"statement_ids":[4],"state":"finished","livy_statement_state":"available","session_id":"cdc93cc4-6ac7-4438-97ae-142997d2a9e8","normalized_state":"finished","queued_time":"2025-05-23T19:57:45.2824412Z","session_start_time":null,"execution_start_time":"2025-05-23T19:57:45.2836964Z","execution_finish_time":"2025-05-23T19:57:55.024407Z","parent_msg_id":"a6765866-1ef9-47b6-998d-815cb83ba86e"},"text/plain":"StatementMeta(, cdc93cc4-6ac7-4438-97ae-142997d2a9e8, 4, Finished, Available, Finished)"},"metadata":{}}],"execution_count":2,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4214a6c9-2142-42b1-affc-92b976540578"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"2aaafc3d-8d61-4a21-99ae-09bb332840ae","known_lakehouses":[{"id":"2aaafc3d-8d61-4a21-99ae-09bb332840ae"}],"default_lakehouse_name":"Lakehouse_E2E","default_lakehouse_workspace_id":"9076d5e4-30ee-441b-8e66-85ac9711a0e2"},"environment":{"environmentId":"31fd62f0-7a25-4de6-8968-114832cf2a84","workspaceId":"9076d5e4-30ee-441b-8e66-85ac9711a0e2"}}},"nbformat":4,"nbformat_minor":5}